{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install list of libraries\n",
    "# %pip install imbalanced-learn\n",
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install scipy\n",
    "# %pip install seaborn --upgrade\n",
    "# %pip install graphviz\n",
    "# %pip install xgboost\n",
    "# %pip install lightgbm\n",
    "# %pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, getcwd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.feature_selection import VarianceThreshold, chi2, f_classif, SelectKBest, SelectFromModel\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, balanced_accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "# from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "subjects = [102, 104, 105, 107, 110, 111, 115, 116, 117, 118, 120, 126, 127, 130, 131, 132, 133, 135, 138, 141, 143, 144]\n",
    "col = ['1','2','3','Label', 'Frontal P3 mean', 'Frontal P3 STD', 'Posterior P3 mean', 'Posterior P3 STD', 'Frontal alpha mean', \n",
    "           'Posterior alpha mean', 'Alpha variability', 'Reaction time Mean', 'Reaction time variability', 'Accuracy', 'Frontal P3 log energy entropy', \n",
    "           'Frontal P3 Shannon entropy', 'Frontal P3 SURE entropy', 'Frontal P3 Skewness', 'Frontal P3 Kurtosis', 'Frontal alpha log energy entropy',\n",
    "           'Frontal alpha Shannon entropy', 'Frontal alpha SURE entropy', 'Frontal alpha Skewness', 'Frontal alpha Kurtosis', \n",
    "           'Posterior P3 log energy entropy', 'Posterior P3 Shannon entropy', 'Posterior P3 SURE entropy', 'Posterior P3 Skewness', 'Posterior P3 Kurtosis', \n",
    "           'Posterior alpha log energy entropy', 'Posterior alpha Shannon entropy', 'Posterior alpha SURE entropy', 'Posterior alpha Skewness',\n",
    "           'Posterior alpha Kurtosis'\n",
    "]\n",
    "cwd = getcwd()\n",
    "target_names = ['Task Unrelated Thought', 'Task Related Thought']\n",
    "results_file = 'Results.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all subject mat files, append TR and TUR structures to dataframe\n",
    "for a in subjects:\n",
    "    file = 'Feature_data_'+str(a)+'.mat'\n",
    "    #Absolute path to mat file:\n",
    "    #loc = os.path.join('C:/Users/pisis/OneDrive - University of Calgary/2024/AIRS/TR and TUT data',file)\n",
    "    loc = path.join(cwd, 'TR and TUT data', file)\n",
    "    subData = loadmat(loc)['data']\n",
    "    subData_TR = subData['TR'][0,0]\n",
    "    subData_TUR = subData['TUR'][0,0]\n",
    "    subDF_TR = pd.DataFrame(subData_TR, columns = col)\n",
    "    subDF_TUR = pd.DataFrame(subData_TUR, columns = col)\n",
    "    if a==subjects[0]:\n",
    "        totalDF = pd.concat([subDF_TR,subDF_TUR])\n",
    "    else:\n",
    "        totalDF = pd.concat([totalDF, subDF_TR])\n",
    "        totalDF = pd.concat([totalDF, subDF_TUR])\n",
    "\n",
    "#Show Data with NaN values:\n",
    "# print(totalDF[totalDF.isnull().any(axis=1)])\n",
    "# NOTE: Subject 109 has NaN values in the Reaction time Mean and Reaction time variability columns. Excluded from analysis.\n",
    "# totalDF.fillna(0, inplace=True)\n",
    "\n",
    "totalDF.reset_index(drop=True, inplace=True)\n",
    "# print(totalDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = totalDF.Label\n",
    "Y = Y - 1\n",
    "all_features = totalDF.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature selection is for determining the most important features in our data. We compared all features, the alpha + posterior features, and finally, the alpha + posterior + behavioural features before using the built-in feature analysis (suggested by Sarah).  \n",
    "[There are many types of feature selection techniques](https://youtu.be/LTE7YbRexl8?si=xW9kJt1lciKEKwAW). \n",
    "1. Filter-based techniques:\n",
    "    - Correlation\n",
    "    - Variance threshold\n",
    "    - Chi squared\n",
    "    - Anova\n",
    "    - Information Gain\n",
    "2. Wrapper techniques:\n",
    "    - Recursive Feature Elimination (RFE)\n",
    "3. Embed techniques\n",
    "    - L1 & L2\n",
    "    - Pruning/Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_columns = ['Posterior P3 mean', 'Posterior P3 STD', 'Posterior alpha mean', 'Alpha variability', 'Accuracy', 'Posterior P3 log energy entropy', \n",
    "              'Posterior P3 Shannon entropy', 'Posterior P3 SURE entropy', 'Posterior P3 Skewness', 'Posterior P3 Kurtosis', \n",
    "              'Posterior alpha log energy entropy', 'Posterior alpha Shannon entropy', 'Posterior alpha SURE entropy', 'Posterior alpha Skewness',\n",
    "              'Posterior alpha Kurtosis']\n",
    "ap_features = totalDF[ap_columns]\n",
    "\n",
    "apb_columns = ['Posterior P3 mean', 'Posterior P3 STD', 'Posterior alpha mean', 'Alpha variability', 'Reaction time Mean', 'Reaction time variability', 'Accuracy',\n",
    "               'Posterior P3 log energy entropy', 'Posterior P3 Shannon entropy', 'Posterior P3 SURE entropy', 'Posterior P3 Skewness', 'Posterior P3 Kurtosis', \n",
    "               'Posterior alpha log energy entropy', 'Posterior alpha Shannon entropy', 'Posterior alpha SURE entropy', 'Posterior alpha Skewness',\n",
    "               'Posterior alpha Kurtosis']\n",
    "apb_features = totalDF[apb_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correlation Matrix:\n",
    "corrMat = all_features.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "# sns.heatmap(corrMat, annot=True, cmap='Blues', fmt=\".2f\")\n",
    "\n",
    "# All features with correlation ge than .80:\n",
    "columns_to_drop = ['Frontal P3 log energy entropy','Frontal alpha log energy entropy', 'Frontal alpha Kurtosis', \n",
    "                   'Posterior P3 log energy entropy', 'Posterior alpha log energy entropy', 'Posterior alpha Kurtosis']\n",
    "uncorr_features = all_features.drop(columns=columns_to_drop, axis=1)\n",
    "# sns.heatmap(uncorr_features.corr(), annot=True, cmap='Blues', fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kendall's Tau Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kenmat = all_features.corr(method='kendall')\n",
    "# sns.heatmap(kenmat, annot=True, cmap='Blues', fmt=\".2f\")\n",
    "\n",
    "# All features with kendall correlation greater than .80\n",
    "columns_to_drop = ['Frontal P3 log energy entropy', 'Posterior P3 log energy entropy']\n",
    "kendall_features = all_features.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features excluded:  ['Accuracy' 'Frontal P3 Kurtosis' 'Frontal alpha Skewness'\n",
      " 'Posterior P3 Skewness' 'Posterior P3 Kurtosis']\n"
     ]
    }
   ],
   "source": [
    "vt = VarianceThreshold(threshold=0.1)\n",
    "vt.fit(all_features)\n",
    "mask = vt.get_support()\n",
    "print(\"Features excluded: \", all_features.columns[~mask].values)\n",
    "vt_features = all_features.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features excluded:  []\n"
     ]
    }
   ],
   "source": [
    "test = SelectKBest(score_func=f_classif, k=30)\n",
    "fit = test.fit(all_features, Y)\n",
    "mask = fit.get_support()\n",
    "print(\"Features excluded: \", all_features.columns[~mask].values)\n",
    "anova_features = all_features.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Feature Selection\n",
    "This is the \"switch\" box if you want to test out a different mode of feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'AP' = alpha + posterior, 'APB' = alpha + posterior + behvaioural, 'all' = all features\n",
    "Select_features = 'uncorr'\n",
    "Notes = ''\n",
    "\n",
    "if Select_features == 'AP':\n",
    "    X = ap_features\n",
    "elif Select_features == 'APB':\n",
    "    X = apb_features\n",
    "elif Select_features == 'uncorr':\n",
    "    X = uncorr_features\n",
    "elif Select_features == 'vt':\n",
    "    X = vt_features\n",
    "elif Select_features == 'ANOVA':\n",
    "    X = anova_features\n",
    "elif Select_features == 'kendalls':\n",
    "    X = kendall_features\n",
    "else: \n",
    "    X = all_features\n",
    "    \n",
    "\n",
    "\n",
    "# print(X.columns)\n",
    "# Verify that Labels contain only 0 and 1:\n",
    "# print(X.Label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer Switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = 3\n",
    "if normalized == 1:\n",
    "    scaler = StandardScaler()\n",
    "elif normalized == 2:\n",
    "    scaler = MinMaxScaler()\n",
    "elif normalized == 3:\n",
    "    scaler = Normalizer()\n",
    "else:\n",
    "    scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables init\n",
    "Choose what type of KFold, which measures you would like, what info you would like to store in results. Some resources:\n",
    "- [svm](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "- [Gradient Boosting info](https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# results = {'Timestamp': [], 'Features': Select_features, 'CrossVal': type(kf).__name__, 'model': [], \"Accuracy\": [], \"BA\": [], \"Matt_Corr_Coef\": [], 'AUC': [], 'CnfM00': [], 'CnfM01': [], \n",
    "#            'CnfM10': [], 'CnfM11': [], 'Notes': Notes}\n",
    "# Fill this with the models you would like to test:\n",
    "# regressors = [LogisticRegression(max_iter=1800, random_state=42), RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), SVC(random_state=42), KNeighborsClassifier(), \n",
    "            #   XGBClassifier(random_state=42), LGBMClassifier(random_state=42), CatBoostClassifier(random_state=42)]\n",
    "# regressors = [CatBoostClassifier(depth= 6, iterations= 300, learning_rate= 0.1, random_state=42), \n",
    "#               RandomForestClassifier(bootstrap= False, max_depth= 40, max_features= \"log2\", min_samples_leaf= 1, \n",
    "#                                      min_samples_split= 2, n_estimators= 200, random_state=42)]\n",
    "# regressors = [RandomForestClassifier(bootstrap= False, max_depth= 40, max_features= \"log2\", min_samples_leaf= 1, \n",
    "#                                      min_samples_split= 2, n_estimators= 200, random_state=42)]\n",
    "# Define the metrics\n",
    "param_results = {}\n",
    "scoring = {\n",
    "    'accuracy': metrics.make_scorer(accuracy_score),\n",
    "    'balanced_accuracy': metrics.make_scorer(balanced_accuracy_score),\n",
    "    'matthews_corrcoef': metrics.make_scorer(matthews_corrcoef),\n",
    "    'AUC': metrics.make_scorer(roc_auc_score),\n",
    "    'confusion_matrix': metrics.make_scorer(confusion_matrix)\n",
    "}\n",
    "models = [\n",
    "    ('RandomForest', RandomForestClassifier(), \n",
    "     {'n_estimators': [100, 200, 300], 'max_depth': [20, 30, 40], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], \n",
    "      'bootstrap': [True, False]}),\n",
    "    ('CatBoost', CatBoostClassifier(), \n",
    "     {'depth': [4, 6, 10], 'iterations': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.5]}),\n",
    "    ('LightGBM', LGBMClassifier(), \n",
    "     {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30, 40], 'learning_rate': [0.01, 0.1, 0.5]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Within-fold processing__:\n",
    "Preprocessing and feature selection should be done within the cross-validation loop to avoid data leakage or overfitting. \n",
    "1. StratifiedKFold.split - split the data into n folds: n-1 training folds and 1 test fold. Run n times with n different test folds. Stratified means the classification ratios in each fold are similar to the ratio in the original dataset. \n",
    "2. SMOTE - increase the number of samples in the minority group synthetically\n",
    "3. Normalization/Standardization - Scale the data, usually to values between 0 and 1. Prevents certain models from adding more weight to larger gaps or higher numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model, param_grid in models:\n",
    "    pipe = Pipeline([('smote', SMOTE(random_state=42)), ('normalizer', scaler)], ['classifier', model])\n",
    "    \n",
    "    inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scoring, refit='balanced_accuracy', cv=inner_cv, n_jobs=-1)\n",
    "    \n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    nested_score = cross_validate(grid_search, X, Y, scoring=scoring, cv=outer_cv, n_jobs=-1)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {nested_score['test_accuracy'].mean()}\")\n",
    "    print(f\"Balanced Accuracy: {nested_score['test_balanced_accuracy'].mean()}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {nested_score['test_matthews_corrcoef'].mean()}\")\n",
    "    print(f\"AUC: {nested_score['test_AUC'].mean()}\")\n",
    "    print(f\"Confusion Matrix: {nested_score['test_confusion_matrix'].sum()}\")\n",
    "    \n",
    "    print(f\"Best model: {grid_search.best_estimator_}\")\n",
    "    print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    param_results[name] = cv_results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Results\n",
    "Add these Results and Test Conditions to Results.xlsx. Will delete duplicate results within conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['Normalized'] = normalized\n",
    "# df_newScores = pd.DataFrame(results)\n",
    "# print(df_newScores)\n",
    "# df_existingRecord = pd.read_excel(results_file)\n",
    "# df_combined = pd.concat([df_existingRecord, df_newScores], ignore_index=True)\n",
    "# df_combined.drop_duplicates(subset=['Features', 'model', \"Accuracy\", \"BA\", \"Matt_Corr_Coef\", 'AUC', 'CnfM00', 'CnfM01', 'CnfM10', 'CnfM11', 'Notes'], keep='last', inplace = True)\n",
    "# df_combined.sort_values(by='Matt_Corr_Coef', ascending=False, inplace=True)\n",
    "# df_combined.to_excel(results_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the presentation. Once model is decided, can use the following to generate graphs showing parameters vs performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "param_results['RandomForest'].plot('param_n_estimators', 'mean_test_score')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Test Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
